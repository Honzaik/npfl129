title: NPFL129, Lecture 6
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Kernel Methods, SVM

## Milan Straka

### November 09, 2020

---
section: KernelLR
# Kernel Linear Regression

Consider linear regression with cubic features
$$φ(→x) = \scriptsize\begin{bmatrix} 1 \\ x_1 \\ x_2 \\ … \\ x_1^2 \\ x_1x_2 \\ … \\ x_2x_1 \\ … \\ x_1^3 \\ x_1^2x_2 \\ … \end{bmatrix}.$$

~~~
The SGD update of a linear regression with batch with indices $→b$ is then
$$→w ← →w - \frac{α}{|→b|}∑_{i ∈ →b}\big(φ(→x_i)^T →w - t_i\big) φ(→x).$$

---
# Kernel Linear Regression

When dimensionality of input is $D$, one step of SGD takes $𝓞(D^3)$.

~~~
Surprisingly, we can do better under some circumstances. We start by
noting that we can write the parameters $→w$ as a linear combination
of the input features $φ(→x_i)$.

~~~
By induction, $→w = 0 = ∑_i 0 ⋅ φ(→x_i)$, and assuming $→w = ∑_i β_i ⋅ φ(→x_i)$,
after a SGD update we get
$$\begin{aligned}
→w ←& →w - \frac{α}{|→b|}∑_{i ∈ →b} \big(φ(→x_i)^T →w - t_i\big) φ(→x_i)\\
   =& ∑_i \Big(β_i + [i ∈ →b] ⋅ \frac{α}{|→b|} \big(t_i - φ(→x_i)^T →w\big)\Big) φ(→x_i).
\end{aligned}$$

~~~
Every $β_i$ for $i ∈ →b$ changes to $β_i + \frac{α}{|→b|}\Big(t_i - φ(→x_i)^T →w\Big)$, so after
substituting for $→w$ we get
$$β_i ← β_i + \frac{α}{|→b|}\Big(t_i - ∑\nolimits_j β_j φ(→x_i)^T φ(→x_j)\Big).$$

---
# Kernel Linear Regression

We can formulate an alternative linear regression algorithm (a so-called
**dual formulation**):

<div class="algorithm">

**Input**: Dataset ($⇉X = \{→x_1, →x_2, …, →x_N\} ∈ ℝ^{N×D}$, $→t ∈ ℝ^N$), learning rate $α ∈ ℝ^+$.<br>

- Set $β_i ← 0$
- Compute all values $K(→x_i, →x_j) = φ(→x_i)^T φ(→x_j)$
- Repeat until convergence
  - Sample a batch $→b$ (usually by generating a random permutation and splitting it)
  - Simultaneously for all $i ∈ →b$ (the $β_j$ on the right side must
    not be modified during the updates):
    - $β_i ← β_i + \frac{α}{|→b|}\Big(t_i - ∑\nolimits_j β_j K(→x_i, →x_j)\Big)$
</div>

~~~
The predictions are then performed by computing
$$y(→z) = φ(→z)^T →w = ∑\nolimits_i β_i →φ(→z)^T →φ(→x_i).$$

---
section: Kernels
# Kernel Trick

A single SGD update of all $β_i$ then takes $𝓞(N^2)$, given that we can
evaluate scalar dot product of $φ(→x_i)^T φ(→x_j)$ quickly.

~~~
$$\begin{aligned}
φ(→x)^T φ(→z) =& 1 + ∑_i x_i z_i + ∑_{i,j} x_i x_j z_i z_j + ∑_{i,j,k} x_i x_j x_k z_i z_j z_k \\
              =& 1 + ∑_i x_i z_i + \Big(∑_i x_i z_i\Big)^2 + \Big(∑_i x_i z_i\Big)^3 \\
              =& 1 + →x^T →z + \big(→x^T →z\big)^2 + \big(→x^T →z\big)^3.
\end{aligned}$$

---
# Kernels

We define a _kernel_ corresponding to a feature map $φ$ as a function
$$K(→x, →z) ≝ φ(→x)^t φ(→z).$$

~~~
There is quite a lot of theory behind kernel construction. The most often used
kernels are the following.

~~~
- _Polynomial kernel or degree $d$_, also called _homogenous polynomial kernel_
  $$K(→x, →z) = (γ →x^T→z)^d,$$
  which corresponds to a feature map generating all combinations of exactly
  $d$ input features.

~~~
- _Polynomial kernel or degree at most $d$_, also calles _nonhomogenous
  polynomial kernel_
  $$K(→x, →z) = (γ →x^T→z + 1)^d,$$
  which corresponds to a feature map generating all combinations of up to
  $d$ input features.

---
# Kernels

- Gaussian (or RBF, Radial basis function) kernel
  $$K(→x, →z) = e^{-γ||→x-→z||^2},$$
  corresponding to a scalar product in an infinite-dimensional space (it is
  a combination of polynomial kernels of all degrees).

---
section: SVM
# Support Vector Machines

Let us return to a binary classification task. The perceptron algorithm
guaranteed finding some separating hyperplane if it existed; we now consider
finding the one with _maximum margin_.

![w=100%,h=center](svm_margin.svgz)

---
# Support Vector Machines

Assume we have a dataset $⇉X ∈ ℝ^{N×D}$, $→t ∈ \{-1, 1\}^N$, feature map $φ$ and model
$$y(→x) ≝ →φ(→x)^T →w + b.$$

~~~
![w=30%,f=right](../03/binary_classification.svgz)

We already know that the distance of a point $→x_i$ to the decision boundary is
$$\frac{|y(→x_i)|}{||→w||} = \frac{t_i y(→x_i)}{||→w||}.$$

~~~
We therefore want to maximize
$$\argmax_{w,b} \frac{1}{||→w||} \min_i \big[t_i (→φ(→x)^T →w + b)\big].$$

However, this problem is difficult to optimize directly.

---
# Support Vector Machines

Because the model is invariant to multiplying $→w$ and $b$ by a constant, we can
say that for the points closest to the decision boundary, it will hold that
$$t_i y(→x_i) = 1.$$

~~~
Then for all the points we will have $t_i y(→x_i) ≥ 1$ and we can simplify
$$\argmax_{w,b} \frac{1}{||→w||} \min_i \big[t_i (→φ(→x)^T →w + b)\big]$$
to
$$\argmin_{w,b} \frac{1}{2} ||→w||^2 \textrm{~given that~~}t_i y(→x_i) ≥ 1.$$

---
section: KKT
# Lagrange Multipliers – Inequality Constraints

Given a funtion $f(→x)$, we can find a maximum with respect to a vector
$→x ∈ ℝ^d$, by investigating the critical points $∇_→x f(→x) = 0$.
We even know how to incorporate constraints of form $g(→x) = 0$.
~~~
We now describe how to include inequality constraints $g(→x) ≥ 0$.

~~~
![w=25%,f=right](lagrange_inequalities.svgz)

We start by again forming a Lagrangian $f(→x) + λg(→x)$.

~~~
The optimum can either be attained for $g(→x) > 0$, when the constraint is said
to be **inactive**, or for $g(→x) = 0$, when the constraint is said to be
**active**.
~~~
In the inactive case, the maximum is again a critical point of the Langrangian
with the condition $λ=0$.

~~~
When maximum is on a boundary, it corresponds to a critical point
with $λ≠0$ – but note that this time the sign of the multiplier matters, because
maximum is attained only when gradient of $f(→x)$ is oriented away from the region
$g(→x) ≥ 0$. We therefore require $∇f(→x) = - λ∇g(→x)$ for $λ>0$.

~~~
In both cases, $λ g(→x) = 0$.

---
section: KKT
# Karush-Khun-Tucker Conditions

![w=25%,f=right](lagrange_inequalities.svgz)

Therefore, the solution to a maximization problem of $f(x)$ subject to $g(→x)≥0$
can be found by inspecting all points where the derivation of the Lagrangian is zero,
subject to the following conditions:
$$\begin{aligned}
g(→x) &≥ 0 \\
λ &≥ 0 \\
λ g(→x) &= 0
\end{aligned}$$

~~~
# Necessary and Sufficient KKT Conditions

The above conditions are necessary conditions for a maximum (resp. minimum). However, it can be
proven that in the following settings, the conditions are also **sufficient**:
- if the objective to optimize is a _concave_ function (resp. _convex_ for minimization);
~~~
- the inequality constraings are continuously differentiable convex functions,
~~~
- the equality constraints are affine functions (linear functions with an
  offset).


---
section: Dual SVM Formulation
# Support Vector Machines

In order to solve the constrained problem of
$$\argmin_{w,b} \frac{1}{2} ||→w||^2 \textrm{~given that~~}t_i y(→x_i) ≥ 1,$$
~~~
we write the Lagrangian with multipliers $→a=(a_1, …, a_N)$ as
$$L = \frac{1}{2} ||→w||^2 - ∑_i a_i \big[t_i y(→x_i) - 1\big].$$

~~~
Setting the derivatives with respect to $→w$ and $b$ to zero, we get
$$\begin{aligned}
→w =& ∑_i a_i t_iφ(→x_i), \\
 0 =& ∑_i a_i t_i. \\
\end{aligned}$$

---
# Support Vector Machines

Substituting these to the Lagrangian, we get
$$L = ∑_i a_i -  \frac{1}{2} ∑_i ∑_j a_i a_j t_i t_j K(→x_i, →x_j)$$
with respect to the constraints $∀_i: a_i ≥ 0$, $∑_i a_i t_i = 0$
and kernel $K(→x, →z) = φ(→x)^T φ(→z).$

~~~
The solution of this Lagrangian will fulfil the KKT conditions, meaning that
$$\begin{aligned}
a_i &≥ 0, \\
t_i y(→x_i) - 1 &≥ 0, \\
a_i \big(t_i y(→x_i) - 1\big) &= 0.
\end{aligned}$$

~~~
Therefore, either a point is on a boundary, or $a_i=0$. Given that the
predictions for point $→x$ are $y(→x) = ∑ a_i t_i K(→x, →x_i) + b$,
we need to keep only the points on the boundary, the so-called **support vectors**.
Even if SVM is nonparametric model, it stores only a subset of data.

---
# Support Vector Machines

The dual formulation allows us to use non-linear kernels.

![w=100%](svm_gaussian.svgz)

---
section: Soft-margin SVN
# Support Vector Machines for Non-linearly Separable Data

![w=28%,f=right](svm_softmargin.svgz)

Until now, we assumed the data to be linearly separable – the  
**hard-margin SVM** variant. We now relax this condition to arrive at
**soft-margin SVM**.
~~~
The idea is to allow points to be in the margin or even on the _wrong side_ of
the decision boundary. We introduce _slack variables_ $ξ_i ≥ 0$, one for each
training instance, defined as
$$ξ_i = \begin{cases} 0 &\textrm{~for points fulfilling~}t_i y(→x_i) ≥ 1,\\
                      |t_i - y(→x_i)|&\textrm{~otherwise}.\end{cases}$$

~~~
Therefore, $ξ_i=0$ signifies a point outside of margin, $0 < ξ_i < 1$ denotes
a point inside the margin, $ξ_i=1$ is a point on the decision boundary and
$ξ_i>1$ indicates the point is on the opposite side of the separating
hyperplane.

~~~
Therefore, we want to optimize
$$\argmin_{w,b} C ∑_i ξ_i + \frac{1}{2} ||→w||^2 \textrm{~given that~~}t_i y(→x_i) ≥ 1 - ξ_i\textrm{~and~}ξ_i ≥ 0.$$

---
# Support Vector Machines for Non-linearly Separable Data

We again create a Lagrangian, this time with multipliers $→a=(a_1, …, a_N)$
and also $→μ=(μ_1, …, μ_N)$:
$$L = \frac{1}{2} ||→w||^2 + C ∑_i ξ_i - ∑_i a_i \big[t_i y(→x_i) - 1 + ξ_i\big] - ∑_i μ_i ξ_i.$$

~~~
Solving for the critical points and substituting for $→w$, $b$ and $→ξ$ (obtaining an additional
constraint $μ_i = C - a_i$ compared to the previous case), we obtain the Lagrangian in the form
$$L = ∑_i a_i -  \frac{1}{2} ∑_i ∑_j a_i a_j t_i t_j K(→x_i, →x_j),$$
which is identical to the previous case, but the constraints are a bit
different:
$$∀_i: C ≥ a_i ≥ 0\textrm{~and~}∑_i a_i t_i = 0.$$

---
# Support Vector Machines for Non-linearly Separable Data

Using KKT conditions, we can see that the support vectors (examples with
$a_i>0$) are the ones with $t_i y(→x_i) = 1 - ξ_i$, i.e., the examples
on the margin boundary, inside the margin and on the opposite side
of the decision boundary.

![w=50%,h=center](svm_softmargin_supports.svgz)

---
# SGD-like Formulation of Soft-Margin SVM

Note that the slack variables can be written as
$$ξ_i = \max\big(0, 1 - t_i y(→x_i)\big),$$
so we can reformulate the soft-margin SVM objective using the **hinge loss**
$$𝓛_\textrm{hinge}(t, y) ≝ \max(0, 1 - t y)$$
to
$$\argmin_{w,b} C ∑_i 𝓛_\textrm{hinge}\big(t_i, y(→x_i)\big) + \frac{1}{2} ||→w||^2 .$$

~~~
Such formulation is analogous to a regularized loss, where $C$ is an _inverse_
regularization strength, so $C=∞$ implies no regularization and $C=0$ ignores
the data entirely.

---
class: tablewide
# Comparison of Linear and Logistic Regression and SVM

For $f(→x; →w, b) ≝ →φ(→x)^T →w + b$, we have seen the following losses:

| Model | Objective | Per-Instance Loss |
|:------|:----------|:------------------|
| Linear<br>Regression | $\small \argmin_{→w,b} ∑_i 𝓛_\textrm{MSE}\big(t_i, f(→x_i)\big) + \frac{1}{2} λ \|→w\|^2$ | $\small 𝓛_\textrm{MSE}(t, y) = \frac{1}{2}(t - y)^2$ |
| Logistic<br>regression | $\small \argmin_{→w,b} ∑_i 𝓛_\textrm{σ-NLL}\big(t_i, f(→x_i)\big) + \frac{1}{2} λ \|→w\|^2$ | $\small 𝓛_\textrm{σ-NLL}(t, y) = - \log \begin{pmatrix}σ(y)^t +\\ \big(1-σ(y)\big)^{1-t}\end{pmatrix}$ |
| Softmax<br>regression | $\small \argmin_{⇉W,→b} ∑_i 𝓛_\textrm{s-NLL}\big(t_i, f(→x_i)\big) + \frac{1}{2} λ \|→w\|^2$ | $\small 𝓛_\textrm{s-NLL}(t, →y) = - \log \softmax(→y)_t$ |
| SVM | $\small \argmin_{→w,b} C ∑_i 𝓛_\textrm{hinge}\big(t_i, f(→x_i)\big) + \frac{1}{2} \|→w\|^2$ | $\small 𝓛_\textrm{hinge}(t, y) = \max(0, 1 - ty)$ |

~~~
Note that $\small 𝓛_\textrm{MSE}(t, y) ∝ -\log\big(𝓝(t; μ=y, σ^2=1)\big)$ and
that $\small 𝓛_\textrm{σ-NLL}(t, y) = 𝓛_\textrm{s-NLL}(t, [y, 0])$.

---
# Binary Classification Loss Functions Comparison

To compare various functions for binary classification, we need to formulate
them all in the same settings, with $t ∈ \{-1, 1\}$.

~~~
- MSE: $(ty - 1)^2$, because it is $(y - 1)^2$ for $t=1$ and $(-y - t)^2$ for $t=-1$,
~~~
- LR: $σ(ty)$, because it is $σ(y)$ for $t=1$ and $1-σ(y)=σ(-y)$ for $t=-1$,
~~~
- SVM: $\max(0, 1 - ty)$.

![w=42%,h=center](binary_losses.svgz)

---
section: SMO
# Sequential Minimal Optimization Algorithm

To solve the dual formulation of a SVM, usually Sequential Minimal Optimization
(SMO; John Platt, 1998) algorithm is used.

~~~
Before we introduce it, we start by introducing **coordinate descent**
optimization algorithm.

~~~
Consider solving unconstrained optimization problem
$$\argmin_→w L(w_1, w_2, …, w_D).$$

~~~
Instead of the usual SGD approach, we could optimize the weights one by one,
using the following algorithm

<div class="algorithm">

- loop until convergence
  - for $i$ in $\{1, 2, …, D\}$:
    - $w_i ← \argmin\nolimits_{w_i} L(w_1, w_2, …, w_D)$
</div>

---
# Sequential Minimal Optimization Algorithm

<div class="algorithm">

- loop until convergence
- for $i$ in $\{1, 2, …, D\}$:
  - $w_i ← \argmin\nolimits_{w_i} L(w_1, w_2, …, w_D)$
</div>

![w=42%,f=right](coordinate_descent.svgz)

If the inner $\argmin$ can be performed efficiently, the coordinate descent can
be fairly efficient.

~~~
Note that we might want to choose $w_i$ in different order, for example
by trying to choose $w_i$ providing the largest decrease of $L$.

~~~
The Kernel linear regression dual formulation was in fact trained
by a coordinate descent – updating a single $β_i$ corresponds
to updating weights for a single example.

---
# Sequential Minimal Optimization Algorithm

In soft-margin SVM, we try to minimize
$$L = ∑_i a_i -  \frac{1}{2} ∑_i ∑_j a_i a_j t_i t_j K(→x_i, →x_j),$$
such that
$$∀_i: C ≥ a_i ≥ 0\textrm{~and~}∑_i a_i t_i = 0.$$

~~~
The KKT conditions for the solution can be reformulated (while staying
equivalent) as
$$\begin{aligned}
  a_i > 0 & ⇒ t_i y(→x_i) ≤ 1,~\textrm{ because }a_i > 0 ⇒ t_i y(→x_i) = 1 - ξ_i\textrm{ and we have }ξ_i ≥ 0,\\
  a_i < C & ⇒ t_i y(→x_i) ≥ 1,~\textrm{ because }a_i < C ⇒ μ_i > 0 ⇒ ξ_i = 0\textrm{ and }t_i y(→x_i) ≥ 1 - ξ_i, \\
  0 < a_i < C & ⇒ t_i y(→x_i) = 1,~\textrm{ a combination of both}.
\end{aligned}$$

---
# Sequential Minimal Optimization Algorithm

At its core, the SMO algorithm is just a coordinate descent.

~~~
It tries to find such $α_i$ fulfilling the KKT conditions – for soft-margin SVM,
KKT conditions are sufficient conditions for optimality (the loss is convex and
inequality constraints affine).

~~~
However, note that because of the $∑a_i t_i = 0$ constraint we cannot optimize
just one $a_i$, because a single $a_i$ is determined from the others.

~~~
Therefore, in each step we pick two $a_i, a_j$ coefficients and try to minimize
the loss while fulfilling the constraints.

~~~
<div class="algorithm">

- loop until convergence (until $∀ i: a_i < C ⇒ t_i y(→x_i) ≥ 1$ and $a_i > 0 ⇒  t_i y(→x_i) ≤ 1$)
  - for $i$ in $\{1, 2, …, D\}$, for $j ≠ i$ in $\{1, 2, …, D\}:
    - $a_i, a_j ← \argmin\nolimits_{a_i, a_j} L(a_1, a_2, …, a_D)$ such that $C ≥ a_i ≥ 0$, $∑_i a_i t_i = 0$
</div>

---
# Sequential Minimal Optimization Algorithm

The SMO is an efficient algorithm, because we can compute the update to
$a_i, a_j$ efficiently, because there exists an closed form solution.

~~~
Assume that we are updating $a_i$ and $a_j$. Then using the condition $∑_k a_k t_k = 0$ we can
write $a_i t_i = -∑_{k≠i} a_k t_k$. Given that $t_i^2 = 1$ and denoting $ζ=-∑_{k≠i, k≠j} a_k t_k$, we get
$$a_i = t_i (ζ-a_j t_j).$$

~~~
Minimizing $L(→a)$ with respect to $a_i$ and $a_j$ then amounts to minimizing
a quadratic function of $a_j$, which has an analytical solution.

~~~
Note that the real SMO algorithm has several heuristics for choosing $a_i, a_j$
such that the $L$ can be minimized the most.

---
# Sequential Minimal Optimization Algorithm Sketch

<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{-1,1\}^N$), kernel $⇉K$, regularization parameter $C$, tolerance $\textit{tol}$,
$\textit{max\_passes\_without\_a\_changing}$ value<br>

- Initialize $a_i ← 0$, $b ← 0$, $\textit{passes} ← 0$
- **while** $\textit{passes} < \textit{max\_passes\_without\_a\_changing}$:
  - $\textit{changed\_as} ← 0$
  - **for** $i$ in $1, 2, …, N$:
    - $E_i ← y(→x_i) - t_i$
    - **if** ($a_i < C$ **and** $t_i E_i < -\textit{tol}$) **or** ($a_i > 0$ **and** $t_i E_i > \textit{tol}$):
      - Choose $j ≠ i$ randomly
      - Update $a_i$, $a_j$ and $b$
      - $\textit{changed\_as} ← \textit{changed\_as} + 1$
  - **if** $\textit{changed\_as} = 0$: $\textit{passes} ← \textit{passes} + 1$
  - **else**: $\textit{passes} ← 0$
</div>

---
# Sequential Minimal Optimization Update Rules

We already know that $a_i = t_i (ζ-a_j t_j).$

~~~
To find $a_j$ optimizing the loss $L$, we use the formula for locating a vertex
of a parabola
$$a_j^\textrm{new} ← a_j - \frac{∂L/∂a_j}{∂^2L/∂a_j^2},$$
which is in fact one Newton-Raphson iteration step.

~~~
Denoting $E_j ≝ y(→x_j) - t_j$, we can compute the first derivative as
$$\frac{∂L}{∂a_j} = t_j (E_i - E_j)$$
and the second derivative as
$$\frac{∂^2L}{∂a_j^2} = 2K(→x_i, →x_j) - K(→x_i, →x_i) - K(→x_j, →x_j).$$

---
# Sequential Minimal Optimization Update Rules

If the second derivative is positive, we know that the vertex is really
a minimum, in which case we get

$$a_j^\textrm{new} ← a_j - t_j\frac{E_i - E_j}{2K(→x_i, →x_j) - K(→x_i, →x_i) - K(→x_j, →x_j)}.$$

~~~
We then clip $a_j$ so that $0 ≤ a_i, a_j ≤ C$, by clipping $a_j$ to range $[L, H]$ with
$$\begin{aligned}
t_i = t_j & ⇒ L = \max(0, a_i + a_j - C), H = \min(C, a_i + a_j) \\
t_i ≠ t_j & ⇒ L = \max(0, a_j - a_i), H = \min(C, C + a_j - a_i).
\end{aligned}$$

~~~
Finally, we set
$$a_i^\textrm{new} ← a_i - t_i t_j\big(a_j^\textrm{new} - a_j).$$

---
# Sequential Minimal Optimization Update Rules

To arrive at the bias update, we consider the KKT condition that
for $0 < a_j^\textrm{new} < C$ it must hold that $t_j y(→x_j) = 1$. Combining it with
with $b = E_j + t_j - ∑_l a_l t_l K(→x_j, →x_l)$, we get the following
value
$$b_j = b - E_j - t_i (a_i^\textrm{new} - a_i)K(→x_i, →x_j) - t_j (a_j^\textrm{new} - a_j)K(→x_j, →x_j).$$

~~~
Analogously for $0 < a_i^\textrm{new} < C$ we get
$$b_i = b - E_i - t_i (a_i^\textrm{new} - a_i)K(→x_i, →x_i) - t_j (a_j^\textrm{new} - a_j)K(→x_j, →x_i).$$

~~~
Finally, if $a_j^\textrm{new}, a_i^\textrm{new} ∈ \{0, C\}$, we know that all values between $b_i$ and $b_j$ fulfil the KKT conditions.
We therefore arrive at the following update for bias:
$$b^\textrm{new} = \begin{cases}
  b_i & \textrm{if~~} 0 < a_i^\textrm{new} < C \\
  b_j & \textrm{if~~} 0 < a_j^\textrm{new} < C \\
  \frac{b_i + b_j}{2} & \textrm{otherwise}.
\end{cases}$$

---
# Sequential Minimal Optimization Algorithm Sketch

<div class="algorithm">

**Input**: Dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{-1,1\}^N$), kernel $⇉K$, regularization parameter $C$, tolerance $\textit{tol}$,
$\textit{max\_passes\_without\_a\_changing}$ value<br>

- Update $a_i$, $a_j$, $b$:
  - Express $a_i$ using $a_j$
  - Find $a_j$ optimizing the loss L quadratic with respect to $a_j$
  - Clip $a_j$ so that $0 ≤ a_i, a_j ≤ C$
  - Compute corresponding $a_i$
  - Compute $b$ matching to updated $a_i$, $a_j$
</div>

---
section: Primal vs Dual
class: tablewide
style: td:nth-child(1) {width: 25%}  td:nth-child(2) {width: 35%}
# Primal versus Dual Formulation

Assume we have a dataset with $N$ training examples, each with $D$ features.
Also assume the used feature map $φ$ generates $F$ features.

| Property | Primal Formulation | Dual Formulation |
|:---------|:-------------------|:-----------------|
| Parameters | $F$ | $N$ |
~~~
| Model size | $F$ | $s⋅D$ for $s$ support vectors |
~~~
| Usual training time | $c ⋅ N ⋅ F$ for $c$ iterations | between $Ω(ND)$ and $𝓞(N^2D)$ |
~~~
| Inference time | $Θ(F)$ | $Θ(s⋅D)$ for $s$ support vectors |
