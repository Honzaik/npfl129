title: NPFL129, Lecture 8
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# SVR, Kernel Approximation,<br>Naive Bayes

## Milan Straka

### November 23, 2020

---
section: SVR
# SVM For Regression

![w=25%,f=right](svr_loss.svgz)

The idea of SVM for regression is to use an $ε$-insensitive error function
$$𝓛_ε\big(t, y(→x)\big) = \max\big(0, |y(→x) - t| - ε\big).$$

~~~
The primary formulation of the loss is then
$$C ∑_i 𝓛_ε\big(t, y(→x)\big) + \frac{1}{2} ||→w||^2.$$

~~~
![w=25%,f=right](svr.svgz)

In the dual formulation, we ideally require every example to be withing $ε$ of
its target, but introduce two slack variables $→ξ^-$, $→ξ^+$ to allow outliers. We therefore
minimize the loss
$$C ∑_i (ξ_i^- + ξ_i^+) + \tfrac{1}{2} ||→w||^2$$
while requiring for every example $t_i - ε - ξ_i^- ≤ y(→x) ≤ t_i + ε + ξ_i^+$ for $ξ_i^- ≥ 0, ξ_i^+ ≥ 0$.

---
# SVM For Regression

The Langrangian after substituting for $→w$, $b$, $→ξ^-$ and $→ξ^+$ is
$$L = ∑_i (a_i^+ - a_i^-) t_i - ε ∑_i (a_i^+ + a_i^-)
      - \frac{1}{2} ∑_i ∑_j (a_i^+ - a_i^-) (a_j^+ - a_j^-) K(→x_i, →x_j)$$

![w=40%,f=right](svr_example.svgz)

subject to
$$\begin{gathered}
  0 ≤ a_i^+, a_i^- ≤ C,\\
  ∑_i(a_i^+ - a_i^-) = 0.
\end{gathered}$$

~~~
The prediction is then given by
$$y(→z) = ∑_i (a_i^+ - a_j^-) K(→z, →x_i) + b.$$

---
section: KernelApproximation
# Using RBF Kernel in Parametric Methods

The RBF kernel empirically works well, but can be used only in the kernel
methods (i.e., in the dual formulation, which is a non-parametric model), which
have time complexity superlinear with the size of the training data.

~~~
Therefore, several methods have been developed to allow using an approximation
of the RBF kernel in parametric models.

---
class: dbend
# Random Fourier Features

One way to approximate RBF kernel is Monte Carlo approximation of its Fourier
transform.

~~~
The Fourier transform of a real-valued integrable function $f$ is
$$f̂(w) ≝ \frac{1}{2π} ∫_{-∞}^∞ f(x) e^{-i x w} \d x,$$
where the $f̂(w)$ can be considered its _frequence spectrum_.

~~~
The transformation is invertible, and we can recover the original function as
$$f(x) = ∫_{-∞}^∞ f̂(w) e^{i x w} \d w.$$

---
class: dbend
# Random Fourier Features

Now consider a shift-invariant kernel $K(→x, →y) = k(→x - →y)$. If we knew its
frequency spectrum $p$, we could write it as
$$k(→x - →y) = ∫_{R^D} p(→w) e^{i →w^T (→x - →y)} \d →w,$$
which we can rewrite using $ξ(→x; →w) = e^{j →w^T →x}$ as
$$k(→x - →y) = ∫_{R^D} p(→w) e^{i →w^T (→x - →y)} \d →w = 𝔼_→w \big[ξ(→x; →w) ξ(→y; →w)^*\big].$$

~~~
Therefore, $ξ(→x; →w) ξ(→y; →w)^*$ is the unbiased estimate of the kernel.

~~~
Because the kernel is real-valued, it is enough to consider the real part of
$ξ$, notably $z(→x; →w, b) = \sqrt 2 \cos(→w^T →x + b)$ for $→w ∼ p$ and $b$
sampled uniformly from $[0, 2π]$, and still get that
$k(→x - →y) = 𝔼_{→w,b} \big[z(→x; →w, b) z(→y; →w, b)\big]$ (by applying expectation
on $b$ using the identity $\cos(→w^T →x + b) \cos(→w^T →y + b) = \tfrac{1}{2}\cos(→w^T (→x-→y)) + \frac{1}{2}\cos(→w^T (→x+→y) + 2b)$.

---
class: dbend
# Random Fourier Features

In order to decrease the variance of the estimator, we sample $M$ values
of $→w$ and $b$ and approximate
$$k(→x - →y) ≈ \frac{1}{M} ∑_{i=1}^M z(→x; →w_i, b_i) z(→y; →w_i, b_i).$$

~~~
Lastly, we need the frequency spectum of an RBF kernel, which is
$$p(→w) = \sqrt{2γ} 𝓝(→w; 0, 1).$$

~~~
The disadvantage of this approach is that we sample completely randomly,
not taking any data into consideration.

---
# Nyström Approximation

A different approach to approximate an RBF kernel is to use a subset
of data as basis.

~~~
Assume that we have a sample of $M$ data $→x_1, …, →x_M$, denoting
$⇉K_{i,j} = k(→x_i, →x_j)$.

~~~
Our goal is to represent $k(→x, →y)$ as
$$k(→x, →y) = ∑_{l=1}^M φ_l(→x) φ_l(→y)$$
by using linear mappings $φ_l(→y) = ⇉V_l^T k(→y, →x_*)$ for a matrix $⇉V$.

~~~
If such identity should hold for all data $→x_1, …, →x_M$, it must hold that
$$⇉K_{i,j} = ∑_l (⇉V_l^T ⇉K_i) ⇉V_l^T ⇉K_j.$$

~~~
We can rewrite the condition for all indices as $⇉K = ⇉K^T ⇉V ⇉V^T ⇉K$.

---
# Nyström Approximation

If e want $⇉K = ⇉K^T ⇉V ⇉V^T ⇉K$ to hold, $⇉V$ should be $⇉K^{-1/2}$.

~~~
Because the kernel is a real symmetric matrix, is has an **eigenvalue decomposition**
$$⇉K = ⇉U ⇉D ⇉U^T,$$
where $⇉U$ is an orthogonal matrix and $⇉D$ is a diagonal one.

~~~
Therefore, we can take
$$⇉V = ⇉U ⇉D^{-1/2} ⇉U^T,$$
which fulfils the required equation. Note that $⇉D^{-1/2}$ can be computed
element-wise, because it is a diagonal matrix.

~~~
The overall kernel is then approximated by measuring the distances of a given
point to the chosen data subset and multiplied by $⇉V$. Empirically, the
approximation works usually better than random Fourier features, because
it concentrates more on the part of the space populated by the data.

---
section: TF-IDF
# Term Frequency – Inverse Document Frequency

To represent a document, we might consider it a **bag of words**, and create
a feature space with a dimension for every word. We can represent a word
in a document as:

- **binary indicators**: 1/0 depending on whether a word is present in
  a document or not;
~~~
- **term frequency (TF)**: relative frequency of a term in a document;
  $$\mathit{TF}(t) = \frac{\textrm{number of occurrences of $t$ in the document}}{\textrm{number of terms in the document}}$$
~~~
- **inverse document frequency (IDF)**: we could also represent a term using
  self-information of a probability of a random document containing it (therefore,
  terms with lower document probability have higher weights);
  $$\mathit{IDF}(t) = \log \frac{\textrm{number of documents}}{\textrm{number of documents containing $t$ }\big(\textrm{optionally} + 1)}$$
~~~
- **TF-IDF**: empirically, product $\mathit{TF} ⋅ \mathit{IDF}$ is a feature
  reflecting quite well how important is a word to a document in a corpus
  (used by 83\% text-based recommender systems in 2015).

---
section: NaiveBayes
# Naive Bayes Classifier

Consider a discriminative classifier modelling probabilities
$$p(C_k|→x) = p(C_k | x_1, x_2, …, x_D).$$

~~~
We might use Bayes' theorem and rewrite it to
$$p(C_k|→x) = \frac{p(C_k) p(→x | C_k)}{p(→x)}.$$

~~~
The so-called **Naive Bayes** classifier assumes all $x_i$
are independent given $C_k$, so we can write
$$p(→x | C_k) = p(x_1 | C_k) p(x_2 | C_k, x_1) p(x_3 | C_k, x_1, x_2) ⋯ p(x_D | C_k, x_1, …)$$
as
$$p(C_k | →x) ∝ p(C_k) ∏_i p(x_i | C_k).$$

---
# Naive Bayes Classifier

There are several used naive Bayes classifiers, depending on the distribution
$p(x_i | C_k)$.

### Gaussian NB

The probability $p(x_i | C_k)$ is modelled as a normal distribution
$𝓝(μ_{i, k}, σ_{i, k}^2)$.

~~~
The parameters $μ_{i,k}$ and $σ_{i,k}^2$ are estimated directly from the data.
However, the variances are usually smoothed (increased) by a given constant $α$
to avoid too sharp distributions.

~~~
- The default value of $α$ in Scikit-learn is $10^9$ times the largest variance
  of all features.

~~~
Gaussian NB is useful if we expect a continuous feature has normal distribution
for a gicen $C_k$.

---
# Naive Bayes Classifier

### Multinomial NB

The probability $p(x_i | C_k)$ is proportional to $p_{i, k}^{x_i}$, so the
$$\log p(C_k, →x) = \log p(C_k) + ∑_i\log p_{i, k}^{x_i} = \log p(C_k) + ∑_i x_i \log p_{i, k} = b + →x^T →w$$
is a linear model in the log space with $b = \log p(C_k)$ and $w_i = \log p_{i, k}$.

~~~
Denoting $n_{i, k}$ as the sum of features $x_i$ for a class $C_k$, the
probabilities $p_{i, k}$ are usually estimated as
$$p_{i, k} = \frac{n_{i, k} + α}{∑_j n_{j, k} + αD}$$
where $α$ is a _smoothing_ parameter accounting for terms not appearing in any
document of class $C_k$ (we can view it as a _pseudocount_ given to every
term in ever document).

---
# Naive Bayes Classifier

### Bernoulli NB

When the input features are binary, the $p(x_i | C_k)$ might also be a Bernoulli
distribution
$$p(x_i | C_k) = p_{i, k}^{x_i} ⋅ (1 - p_{i, k})^{(1-x_i)},$$
and as in the Multinomial NB case, we can write
$$\log p(C_k, →x) = \log p(C_k) + ∑\nolimits_i \big(x_i \log \tfrac{p_{i, k}}{1-p_{i,k}} + \log(1-p_{i,k})\big) = b + →x^T →w.$$
~~~
Similarly to multinomial NB, the probabilities are usually estimated as
$$p_{i, k} = \frac{\textrm{number of documents of class $k$ with nonzero feature $i$} + α}{\textrm{number of documents of class $k$} + 2α}.$$

~~~
The difference with respect to Multinomial NB is that Bernoulli NB explicitly
models also the _absence of terms_ by $(1-p_{i,k})$, while $p_{i,k}^0=1$ is used
in Multinomial NB. However, the cost is that the input features must be binary
(so for example TF-IDF cannot be used).

---
section: GenerativeAndDiscriminative
# Naive Bayes Classifier as a Generative Model

Given that a Multinomial/Bernoulli NB fits $\log p(C_k, →x)$ as a linear model and
a logistic regression also fits $\log p(C_k | →x)$ as a linear model, naive Bayes and
logistic regression form a so-called **generative-discriminative** pair, where
the naive Bayes is a **generative** model, while logistic regression is
a **discriminative** model.

---
# Generative and Discriminative Models

So far, most of our models have been **discriminative**, modeling a _conditional
distribution_ $p(→t | →x)$ (predicting some output distribution). Empirically,
such models usually perform better in classification tasks, but because they do
not estimate the probability of $→x$, it might be difficult for them to
recognize outliers (out-of-distribution data).

~~~
On the other hand, the **generative** models estimate a _joint distribution_
$p(→t, →x)$, often by employing Bayes' theorem and estimating
$p(→x | →t) ⋅ p(→t)$. They therefore model the probability of the data being
generated by an outcome, and only transform it to $p(→t|→x)$ during prediction.

~~~
The term generative comes from a (theoretical) possibility of “generating”
random instances (either of $(→x, →t)$ or $→x$ given $→t$). However, just
being able to evaluate $p(→x | →t)$ does not necessarily mean there must be an
efficient procedure of actually sampling (generating) $→x$.

~~~
In recent years, generative modeling combined with deep neural networks created
a new family of _deep generative models_ like VAE or GAN, which can in fact
efficiently generate samples from $p(→x)$.

---
section: MAP
# Maximum A Posteriori Estimation

We already discussed maximum likelihood estimation
$$→w_\mathrm{MLE} = \argmax_→w p(𝕏; →w).$$

~~~
Instead, we may want to maximize _maximum a posteriori (MAP)_ point estimate:
$$→w_\mathrm{MAP} = \argmax_→w p(→w; 𝕏)$$

~~~
Using Bayes' theorem
$$p(→w; 𝕏) = \frac{p(𝕏; →w) ⋅ p(→w)}{p(𝕏)},$$
we get
$$→w_\mathrm{MAP} = \argmax_→w p(𝕏; →w) ⋅ p(→w).$$

---
# L2 Regularization as MAP

Another way to arrive at L2 regularization is to employ the MAP estimation
and assume that the prior probabilities $p(→w)$ of the parameter values (our
_preference_ among the models) is $𝓝(→w; 0, σ^2)$.

~~~
Then
$$\begin{aligned}
→w_\mathrm{MAP} &= \argmax_→w p(𝕏; →w) ⋅ p(→w) \\
                &= \argmax_→w ∏\nolimits_{i=1}^N p(→x_i; →w) ⋅ p(→w) \\
                &= \argmin_→w ∑\nolimits_{i=1}^N -\log p(→x_i; →w) - \log p(→w) \\
\end{aligned}$$

~~~
By substituting the probability of the Gaussian prior, we get
$$→w_\mathrm{MAP} = \argmin_→w ∑_{i=1}^N -\log p(→x_i; →w) {\color{gray} + \frac{D}{2} \log(2πσ^2)} + \frac{||→w||^2}{2σ^2}.$$
