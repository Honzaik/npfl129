title: NPFL129, Lecture 13
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Statistical Hypothesis Testing, Model Comparison

## Milan Straka

### January 04, 2021

---
section: StatisticalTesting
# Statistical Hypothesis Testing

Variation of a famous saying states, that there are various kinds of truth:

~~~
- truth,

~~~
- half-truth,
~~~
- lie,
~~~
- disgusting lie,
~~~
- and statistics.

---
# Statistical Hypothesis Testing

Assume we have a hypothesis testable using observed data of random variables.

~~~
There are two slightly differing views on statistical hypothesis testing:

~~~
1. In the first one, we assume we have a **null hypothesis** $H_0$, and we
   are interested in whether we can **reject it** using the observed data.

~~~
   The result is **statistically significant**, if it is very unlikely
   that the observed data have occurred given the null hypothesis.

~~~
   The **significance level** of a test is the threshold of this unlikeliness.

~~~
2. In the second view, we have two hypotheses, a null hypothesis $H_0$
   and an **alternate hypothesis** $H_1$, and we want to distinguish among
   them.

~~~
   We consider only two outcomes of the test:
   - either we “reject” the null hypothesis, if the data is very unlikely
   to have occurred given the null hypothesis; or
~~~
   - we cannot reject the null hypothesis.

~~~
   Note that we never “prove” the alternative hypothesis.

---
class: tablewide
# Type I and Type II Errors

Consider the _courtroom trial_ example, which is similar to a criminal trial,
where the defendant is considered not guilty until their guilt is proven.

~~~
In this setting, $H_0$ is “not guilty” and $H_1$ is “guilty”.

~~~
| | $H_0$ is true<br>Truly not guilty | $H_1$ is true<br>Truly guilty |
|-|-----------------------------------|-------------------------------|
| Not proven guilty<br>Not rejecting $H_0$ | Correct decision<br>True negative | Wrong decision<br>False negative<br>**Type II Error** |
| Proven guilty<br>Rejecting $H_0$ | Wrong decision<br>False positive<br>**Type I Error** | Correct decision<br>True positive |

~~~
Our goal is to limit the Type 1 errors – the test **significance level** is the
type 1 error rate.

---
# Match Analogy

I like the following analogy – if you have a theory and want to covince others
that it holds, you devise an _opponent_ for it and let them wrestle.

~~~
If your theory wins, it may be an indication that it really holds.

~~~
However, you must choose an appropriate opponent.

![w=49%](opponent_weak.png)
~~~
![w=44%,mw=49%,h=center](opponent_strong.png)

---
# Statistical Hypothesis Testing

The crucial part of a statistical test is the **test statistic**. It is some
statistic capable of distinguishing the null and alternative hypotheses,
usually a single value (some summary of the observed data) capable of performing
the statistical test.

~~~
It is crucial to be able to compute the distribution of the test statistic,
which allows the **p-values** to be calculated.

~~~
A **p-value** is the probability of obtaining results at least as extreme
as the ones actually observed. A very small p-value indicates that the
observed data are very unlikely under the null hypothesis.

~~~
Given a test statistic, we usually perform one of

![w=11%,f=right](test_right_tail.svgz)

- a one-sided right-tail test, when the p-value of $t$ is $P(\mathrm{test~statistic} > t | H_0)$;

~~~
![w=11%,f=right;clear: both](test_left_tail.svgz)

- a one-sided left-tail test, when the p-value of $t$ is $P(\mathrm{test~statistic} < t | H_0)$;
~~~
![w=11%,f=right;clear: both](test_two_sided.svgz)

- a two-sides test, when the p-value of $t$ is twice the minimum of
  $P(\mathrm{test~statistic} < t | H_0)$ and $P(\mathrm{test~statistic} > t | H_0)$.

---
# Statistical Hypothesis Testing

Therefore, the whole procedure consists of the following steps:
1. Compute the observed value of the test statistics.
~~~
1. Calculate the p-value, which is the probability of a test
   statistic sample at least as extreme as the observed one,
   under the null hypothesis $H_0$.
~~~
1. We reject the null hypothesis $H_0$ (in favor of the alternative
   hypothesis $H_1$), if the p-value is less than the chosen
   significance level $α$ (0.5% and 0.1% are the common choices).

---
# Test Statistics

There are several kinds of test statistics:
~~~
- **one-sample tests**, where we sample values from one distribution.

~~~
  Common one-sample tests usually check for
  - the mean of the distribution to be greater/ than and/or equal to zero;
  - the goodness of fit (that the data comes from a normal or categorical
    distribution of given parameters).

~~~
- **two-sample tests**, where we sample independently from two distributions.

~~~
- **paired tests**, in which case we also sample from two distributions, but
  the samples are paired (i.e., evaluating several models on the same data).

~~~
  In paired tests, we usually compute the difference between the paired members
  and perform one-sample test on the mean of the differences.

---
# Test Statistics

There are many commonly used test statistics, with different requirements
and conditions. We only mention several commonly-used ones, but it is by no means
a comprehensive treatment.

~~~
## Z-Test

~~~
## T-Test

~~~
## Chi-squared Test

~~~
## F-Test

---
section: MultipleComparisons
# Multiple Comparisons Problem

A **multiple comparisons problem** (or multiple testing problem) arises, if we
consider many statistical hypotheses tests using the same observed data.

~~~
![w=72%,h=center](xkcd_significant_1.png)

---
# Multiple Comparisons Problem

![w=70%,h=center](xkcd_significant_2.png)

---
# Multiple Comparisons Problem

![w=70%,h=center](xkcd_significant_3.png)

---
# Multiple Comparisons Problem

![w=90%,mw=45%,f=left](xkcd_significant_4.png)

~~~
Usually, the problem is that we perform many statistical tests and only report
the ones with statistically significant results.

~~~
![w=54%,mh=60%,v=bottom](spurious_correlation.svgz)

---
section: FWER
# Family-Wise Error Rate

There are several ways to handle the multiple comparison problem; one of the
easiest (but often overly conservative) is to limit the **family-wise error
rate**, which is the probability of at least one type 1 error in the family.
$$\mathrm{FWER} = P\bigg(⋃_i \Big(p_i ≤ α\Big)\bigg).$$

~~~
One way of controlling the family-wise error rate is the **Bonferroni
correction**, which rejects the null hypothesis of a test in the family of
size $m$ when $p_i ≤ \frac{α}{m}$.

~~~
Assuming such a correction and utilizing the Boole's inequality
$P\big(⋃\nolimits_i A_i\big) ≤ ∑_i P(A_i)$, we get that
$$\mathrm{FWER} = P\bigg(⋃_i \Big(p_i ≤ \frac{α}{m}\Big)\bigg) ≤ ∑_i P\Big(p_i ≤ \frac{α}{m}\Big) = m \cdot \frac{α}{m} = m.$$

~~~
Note that there exist other more powerful methods like Holm-Bonferroni or Šidák correction.

---
section: ModelComparison
# Model Comparison

WIP

---
section: BootstrapResampling
# Bootstrap Resampling

WIP

---
section: BootstrapPairTest
# Bootstrap Pair Test

WIP

---
# Bootstrap Pair Test

For illustration, consider models for the `isnt_it_ironic` competition
utilizing either 3, 4 or 5 in-word character n-grams.

![w=49%](bootstrap_char3_vs_char4_500.svgz)![w=49%](bootstrap_char3_vs_char4_500_differences.svgz)

---
# Bootstrap Pair Test

For illustration, consider models for the `isnt_it_ironic` competition
utilizing either 3, 4 or 5 in-word character n-grams.

![w=49%](bootstrap_char3_vs_char4_5000.svgz)![w=49%](bootstrap_char3_vs_char4_5000_differences.svgz)

---
# Bootstrap Pair Test

For illustration, consider models for the `isnt_it_ironic` competition
utilizing either 3, 4 or 5 in-word character n-grams.

![w=49%](bootstrap_char4_vs_char5_5000.svgz)![w=49%](bootstrap_char4_vs_char5_5000_differences.svgz)
